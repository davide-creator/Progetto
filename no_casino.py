# -*- coding: utf-8 -*-
"""no casino.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CICe6xI-anYeu9I_J6S4yBHLaDsIbMpA
"""

from google.colab import drive
drive.mount('/content/drive')

from bs4 import BeautifulSoup
import concurrent.futures
import requests
import requests.exceptions
import urllib.parse
from collections import deque
import re
import pandas as pd
from openpyxl import load_workbook
import concurrent.futures
from openpyxl.workbook import Workbook


GIRI_LINKS =49
#skip = 3401 # devi ripartire da qui
skip = 8826
#file = 'amazon (28).xlsx'
file = r'/content/drive/MyDrive/file senzacasino.xlsx'
no_email = '/content/drive/MyDrive/colabNoMail.xlsx'
filePath = r'/content/drive/MyDrive/1.xlsx'
db = pd.read_excel(filePath)
numero_link = len(db)
#db = db.loc[skip:4]
db = db.loc[0:15580]
#db = db.loc[skip-3:15580]

book = Workbook()
test = load_workbook(file)
sheet = test.active
nessuna = load_workbook(no_email)
shet = nessuna.active

def no_link(a):
    jpg=''
    for letter in list(a):
        if letter != '?':
            jpg += letter
        else:
            break
    return jpg


casino = r'/content/drive/MyDrive/CASINO.xlsx'
df = pd.read_excel(casino)
lista_link = list(df['sito'])


def scraper(x, turns):
    user_url = x['link']
    job = x['job']
    state = x['nation']
    city = x['city']
    company = x['company']
    linkedin = x['linkedin']
    description = x['description']
    urls = deque([user_url])
    scraped_urls = set()
    emails = set()
    count = 0
    mailto = []
    finito = []
    try:
        if user_url not in lista_link:
            try:
                while len(urls):
                    count += 1
                    if count == turns:
                        break
                    url = urls.popleft()
                    scraped_urls.add(url)
                    parts = urllib.parse.urlsplit(url)
                    base_url = '{0.scheme}://{0.netloc}'.format(parts)
                    path = url[:url.rfind('/') + 1] if '/' in parts.path else url
                    print('[%d] Processing %s' % (count, url))
                    try:
                        response = requests.get(url, headers={"User-Agent": "XY"})
                    except (requests.exceptions.MissingSchema, requests.exceptions.ConnectionError):
                        continue
                    new_emails = set(re.findall(r'[a-z0-9\.\-+_]+@[a-z0-9\.\-+_]+\.[a-z]+', response.text, re.I))
                    emails.update(new_emails)
                    # mailTo
                    try:
                        sp = BeautifulSoup(response.content, 'html.parser')
                        aa = sp.find_all('a')
                        for x in aa:
                            x = x.get('href')
                            try:
                                if 'mailto' in x:
                                    j = len(x)
                                    x = x[7:j]
                                    mailto.append(x.lower())
                            except:
                                a = 9
                    except: a=9
                    soup = BeautifulSoup(response.text, features='lxml')
                    for anchor in soup.find_all('a'):
                        link = anchor.attrs['href'] if 'href' in anchor.attrs else ''
                        if link.startswith('/'):
                            link = base_url + link
                        elif not link.startswith('http'):
                            link = path + link
                        if not link in urls and not link in scraped_urls:
                            urls.append(link)
            except KeyboardInterrupt:
                print('[-] closing')
            for x in emails:
                if x not in mailto: mailto.append(x.lower())
            mailto = set(mailto)
            if len(mailto)>0:
                for x in mailto:
                    jpg = ''
                    l = list(x)
                    cc = l[len(l) - 3:len(l)]
                    for letter in cc:
                        jpg += str(letter)
                    if jpg != 'jpg':
                        if jpg != 'png':
                            if '%' not in x:
                                if '?'not in x:
                                    finito.append(x)
                                else:
                                    x = no_link(x)
                                    finito.append(x)
                    else:
                        print('era una jpg ', x)
            set(finito)

            if len(finito)>0:
                for x in finito:
                  sheet.append([state, city, x, user_url, job, company, linkedin, description])
                  test.save(file)
            else:
                print('no mail in:  ', user_url)
                shet.append([state, city, user_url, job, company, linkedin, description])
                nessuna.save(no_email)

            print(finito, ' link:  ', user_url, 'numero di mail: ', len(finito))


            return finito
        else:
            return 'link già analizzato'
    except: return 'è successo qualche casino'
contatore = 0
with concurrent.futures.ThreadPoolExecutor() as executor:
    if __name__ == '__main__':
        for x in range(9941, numero_link-1):
            panda_dato = db.loc[x]
            results = [executor.submit(scraper, panda_dato , GIRI_LINKS)]

        for f in concurrent.futures.as_completed((results)):
            contatore+=1
            try:
                print(f.result)
                print(contatore, '/', numero_link)
            except: print('error')