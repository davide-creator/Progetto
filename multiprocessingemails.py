# -*- coding: utf-8 -*-
"""multiprocessingEmails.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t4DxU1CoYkR-7_r2GaEePBioN0xg11X9
"""

from bs4 import BeautifulSoup
import requests
import requests.exceptions
import urllib.parse
from collections import deque
import re
import pandas as pd
from openpyxl import load_workbook
import concurrent.futures
from openpyxl.workbook import Workbook

GIRI_LINKS = 49
skip = 302
#file = '/content/drive/MyDrive/colabTest.xlsx' # su colab lo chiamo in un altro modo

file = '/content/drive/MyDrive/recap 5marzo.xlsx'
no_email = '/content/drive/MyDrive/colabNoMail.xlsx'# su colab lo chiamo in un altro modo
filePath = r'/content/drive/MyDrive/1.xlsx'
db = pd.read_excel(filePath)
numero_link = len(db)
#db = db.loc[skip:15584]
db = db.loc[0:15584]
book = Workbook()
test = load_workbook(file)
sheet = test.active
nessuna = load_workbook(no_email)
shet = nessuna.active

from google.colab import drive
drive.mount('/content/drive')

skip = 15584-11493 # numero di righe che dobbiamo ancora analizzare prima di arrivare a zero (partendo dalla fine del database e arrivando a zero)
skipp = 11493

def no_link(a):
    jpg=''
    for letter in list(a):
        if letter != '?':
            jpg += letter
        else:
            break
    return jpg


def scraper(x, turns):
    user_url = x['link']
    job = x['job']
    state = x['nation']
    city = x['city']
    company = x['company']
    linkedin = x['linkedin']
    description = x['description']
    urls = deque([user_url])
    scraped_urls = set()
    emails = set()
    count = 0
    mailto = []
    finito = []
    try:
        while len(urls):
            count += 1
            if count == turns:
                break
            url = urls.popleft()
            scraped_urls.add(url)
            parts = urllib.parse.urlsplit(url)
            base_url = '{0.scheme}://{0.netloc}'.format(parts)
            path = url[:url.rfind('/') + 1] if '/' in parts.path else url
            print('[%d] Processing %s' % (count, url))
            try:
                response = requests.get(url, headers={"User-Agent": "XY"})
            except (requests.exceptions.MissingSchema, requests.exceptions.ConnectionError):
                continue
            new_emails = set(re.findall(r'[a-z0-9\.\-+_]+@[a-z0-9\.\-+_]+\.[a-z]+', response.text, re.I))
            emails.update(new_emails)
            # mailTo
            try:
                sp = BeautifulSoup(response.content, 'html.parser')
                aa = sp.find_all('a')
                for x in aa:
                    x = x.get('href')
                    try:
                        if 'mailto' in x:
                            j = len(x)
                            x = x[7:j]
                            mailto.append(x.lower())
                    except: 
                        a = 9
            except: a=9
            soup = BeautifulSoup(response.text, features='lxml')
            for anchor in soup.find_all('a'):
                link = anchor.attrs['href'] if 'href' in anchor.attrs else ''
                if link.startswith('/'):
                    link = base_url + link
                elif not link.startswith('http'):
                    link = path + link
                if not link in urls and not link in scraped_urls:
                    urls.append(link)
    except KeyboardInterrupt:
        print('[-] closing')
    for x in emails:
        if x not in mailto: mailto.append(x.lower())
    mailto = set(mailto)
    if len(mailto)>0:
        for x in mailto:
            jpg = ''
            l = list(x)
            cc = l[len(l) - 3:len(l)]
            for letter in cc:
                jpg += str(letter)
            if jpg != 'jpg':
                if jpg != 'png':
                    if '%' not in x:
                        if '?'not in x:
                            finito.append(x)
                        else:
                            x = no_link(x)
                            finito.append(x)
            else:
                print('era una jpg ', x)
    set(finito)

    if len(finito)>0:
        for x in finito:
            sheet.append([state, city, x, user_url, job, company, linkedin, description])
            test.save(file)
    else:
        print('no mail in:  ', user_url)
        shet.append([state, city, user_url, job, company, linkedin, description])
        nessuna.save(no_email)

    print(finito, ' link:  ', user_url, 'numero di mail: ', len(finito))


    return finito

contatore = 0
with concurrent.futures.ProcessPoolExecutor() as executor:
    if __name__ == '__main__':
        for x in range(numero_link-(numero_link - skipp)):
            panda_dato = db.loc[numero_link-x-skip-1]
            results = [executor.submit(scraper, panda_dato , GIRI_LINKS)]

        for f in concurrent.futures.as_completed((results)):
            contatore+=1
            try:
                print(f.result)
                print(contatore, '/', numero_link)
            except: print('error')

from bs4 import BeautifulSoup
import requests.exceptions
import urllib.parse
from collections import deque
import re

user_url = str(input('URL to be scanned....'))
urls = deque([user_url])

scraped_urls = set()
emails = set()

count = 0
try:
    while len(urls):
        count += 1
        if count == 30:
            break
        url = urls.popleft()
        scraped_urls.add(url)
        parts = urllib.parse.urlsplit(url)
        base_url = '{0.scheme}://{0.netloc}'.format(parts)

        path = url[:url.rfind('/') + 1] if '/' in parts.path else url
        print('[%d] Processing %s' % (count, url))
        try:
            response = requests.get(url)
        except (requests.exceptions.MissingSchema, requests.exceptions.ConnectionError):
            continue
        new_emails = set(re.findall(r'[a-z0-9\.\-+_]+@[a-z0-9\.\-+_]+\.[a-z]+', response.text, re.I))
        emails.update(new_emails)
        soup = BeautifulSoup(response.text, features='lxml')
        for anchor in soup.find_all('a'):
            link = anchor.attrs['href'] if 'href' in anchor.attrs else ''
            if link.startswith('/'):
                link = base_url + link
            elif not link.startswith('http'):
                link = path + link
            if not link in urls and not link in scraped_urls:
                urls.append(link)
except KeyboardInterrupt:
    print('[-] closing')

for x in emails:
    print(x)

